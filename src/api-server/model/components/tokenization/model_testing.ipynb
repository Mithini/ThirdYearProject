{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Data from https://raw.githubusercontent.com/malcolmosh/goodbooks-10k/master/books_enriched.csv...\n",
      "Date import complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from model.components.preprocessors.data_preprocessor_v2 import DataPreprocessor\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Set the float format\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "\"\"\"# Import Data\"\"\"\n",
    "filepath = 'https://raw.githubusercontent.com/malcolmosh/goodbooks-10k/master/books_enriched.csv'\n",
    "print(f'Importing Data from {filepath}...')\n",
    "# Import data from the goodbooks-10k repo\n",
    "books_df = pd.read_csv(filepath, index_col=[0], converters={\"genres\": literal_eval})\n",
    "books_ratings = pd.read_csv(filepath)\n",
    "print('Date import complete.\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T19:34:34.570086400Z",
     "start_time": "2024-05-13T19:34:29.772980900Z"
    }
   },
   "id": "6e6c83b94a86a032"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased\"):\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "        self.model = TFDistilBertModel.from_pretrained(model_name)\n",
    "\n",
    "    def extract_features(self, books_df_processed):\n",
    "        document_embeddings = []\n",
    "        for author, title, desc, genres in zip(books_df_processed['author'], books_df_processed['title'],\n",
    "                                               books_df_processed['description'], books_df_processed['genres']):\n",
    "            # Concatenate author, title, and description\n",
    "            input_text = author + ' ' + title + ' ' + desc\n",
    "            genre_text = ' '.join(genres)\n",
    "            input_text = input_text + ' ' + genre_text\n",
    "\n",
    "            # Tokenize input text\n",
    "            inputs = self.tokenizer(input_text, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "\n",
    "            # Forward pass through BERT model\n",
    "            outputs = self.model(inputs)\n",
    "\n",
    "            # Extract embeddings\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "            # You can choose to use the embedding of the [CLS] token or pool the embeddings to get a single vector\n",
    "            pooled_embedding = tf.reduce_mean(last_hidden_states, axis=1)\n",
    "            document_embeddings.append(pooled_embedding.numpy())\n",
    "\n",
    "        # Combine document embeddings with other features\n",
    "        ##language_features = pd.get_dummies(books_df_processed['language_code']).values\n",
    "        composite_feature_vector = np.vstack([document_embeddings])\n",
    "\n",
    "        return composite_feature_vector"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T19:34:39.262978200Z",
     "start_time": "2024-05-13T19:34:39.243924100Z"
    }
   },
   "id": "d8ca0f86d0589c48"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Preprocessing...\n",
      "Preprocessing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Third Year Project\\ThirdYearProject\\src\\api-server\\model\\components\\preprocessors\\data_preprocessor_v2.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  books_df_subset['author'] = books_df_subset['authors'].apply(lambda x: x[0]).astype(str)\n",
      "D:\\Third Year Project\\ThirdYearProject\\src\\api-server\\model\\components\\preprocessors\\data_preprocessor_v2.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  books_df_subset['description'] = books_df_subset['description'].fillna('')\n",
      "D:\\Third Year Project\\ThirdYearProject\\src\\api-server\\model\\components\\preprocessors\\data_preprocessor_v2.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  books_df_subset['title'] = books_df_subset['title'].fillna('')\n",
      "D:\\Third Year Project\\ThirdYearProject\\src\\api-server\\model\\components\\preprocessors\\data_preprocessor_v2.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  books_df_subset['title'] = books_df_subset['title'].str.lower()\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# Preprocessing\"\"\"\n",
    "print('Performing Preprocessing...')\n",
    "preprocessor = DataPreprocessor()\n",
    "books_df_processed = preprocessor.preprocess(books_df)\n",
    "\n",
    "indices = pd.Series(books_df_processed.index, index=books_df_processed['title']).drop_duplicates()\n",
    "print('Preprocessing complete.\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T19:34:42.715302800Z",
     "start_time": "2024-05-13T19:34:42.693901900Z"
    }
   },
   "id": "a66639d4cd55c3ec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Feature Extraction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SGSey\\Documents\\GitHub\\ThirdYearProject\\src\\api-server\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\SGSey\\Documents\\GitHub\\ThirdYearProject\\src\\api-server\\venv\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\SGSey\\Documents\\GitHub\\ThirdYearProject\\src\\api-server\\venv\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# Feature Extraction\"\"\"\n",
    "print('Performing Feature Extraction...')\n",
    "featureExtractor = FeatureExtractor()\n",
    "composite_feature_vector = featureExtractor.extract_features(books_df_processed)\n",
    "print('Feature Extraction complete.\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-05-13T19:34:51.506911800Z"
    }
   },
   "id": "99a4fd767d8d7ec6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"# Similarity Measure\"\"\"\n",
    "print('Generating Similarity Measures...')\n",
    "# Using Cosine Similarity\n",
    "cosine_sim = cosine_similarity(composite_feature_vector)\n",
    "print('Similarity Measure generation complete.\\n')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae7025570837a738"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def recommend_items(title, similarity_measure=cosine_sim, fuzzy=False):\n",
    "    # Convert input title to lowercase\n",
    "    title = title.lower()\n",
    "\n",
    "    # Get the index of the item that matches the title\n",
    "    idx = indices[title]\n",
    "\n",
    "    # Get the pairwise similarity scores of all items with that item\n",
    "    sim_scores = list(enumerate(similarity_measure[idx]))\n",
    "\n",
    "    # Sort the items based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar items\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # Get the item indices\n",
    "    item_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Return the top 10 most similar items\n",
    "    return books_df_processed['title'].iloc[item_indices]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20c221274dd10e35"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
